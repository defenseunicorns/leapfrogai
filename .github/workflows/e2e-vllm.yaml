# End-to-end testing that deploys and tests Supabase, API, UI, and VLLM

name: e2e-vllm
on:
  pull_request:
    types:
      - opened # default trigger
      - reopened # default trigger
      - synchronize # default trigger
      - ready_for_review # don't run on draft PRs
      - milestoned # allows us to trigger on bot PRs
    paths:
      # Catch-all
      - "**"

      # Ignore updates to the .github directory, unless it's this current file
      - "!.github/**"
      - ".github/workflows/e2e-vllm.yaml"
      - ".github/actions/uds-cluster/action.yaml"

      # Ignore docs and website things
      - "!**.md"
      - "!docs/**"
      - "!adr/**"
      - "!website/**"
      - "!netlify.toml"

      # Ignore updates to generic github metadata files
      - "!CODEOWNERS"
      - "!.gitignore"
      - "!LICENSE"

      # Ignore local development files
      - "!.pre-commit-config.yaml"

      # Ignore non e2e tests changes
      - "!tests/pytest/**"

      # Ignore LFAI-UI source code changes
      - "!src/leapfrogai_ui/**"

      # Ignore changes to unrelated packages
      - "!packages/k3d-gpu/**"
      - "!packages/llama-cpp-python/**"
      - "!packages/repeater/**"
      - "!packages/text-embeddings/**"
      - "!packages/ui/**"
      - "!packages/whisper/**"

concurrency:
  group: e2e-vllm-${{ github.ref }}
  cancel-in-progress: true

jobs:
  e2e_vllm:
    runs-on: ai-linux-x64-gpu
    # if: ${{ !github.event.pull_request.draft }}

    steps:
      - name: Checkout Repo
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - name: Setup and Check NVIDIA Dependencies
        run: |
          # Check the NVIDIA driver and container toolkit versions
          nvidia-smi
          nvidia-ctk --version

          # Configure Docker with the NVIDIA Container Toolkit
          sudo nvidia-ctk runtime configure --runtime=docker --set-as-default
          sudo systemctl restart docker

          # Check Docker GPU info and access
          docker info | grep nvidia
          docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi

      - name: Setup Python
        uses: ./.github/actions/python
        with:
          additionalOptionalDep: dev-vllm

      - name: Setup UDS Environment
        uses: defenseunicorns/uds-common/.github/actions/setup@822dac4452e6815aadcf09f487406ff258756a0c # v0.14.0
        with:
          registry1Username: ${{ secrets.IRON_BANK_ROBOT_USERNAME }}
          registry1Password: ${{ secrets.IRON_BANK_ROBOT_PASSWORD }}
          ghToken: ${{ secrets.GITHUB_TOKEN }}
          udsCliVersion: 0.14.0
        continue-on-error: true # see next step for permissions issues workarounds

      - name: Setup UDS Workaround
        env:
          UDS_VERSION: v0.14.0
        run: |
          # Use root permissions to allow installation of UDS CLI binary
          sudo wget -O uds https://github.com/defenseunicorns/uds-cli/releases/download/${{ env.UDS_VERSION }}/uds-cli_${{ env.UDS_VERSION }}_Linux_amd64 && \
          sudo chmod +x uds && \
          sudo chown $(whoami) uds

          # Change ownership of binaries to current runner user
          sudo mv uds /usr/local/bin/
          sudo chown $(whoami) /usr/local/bin/k3d

          # Check runner user access to the binaries
          uds version
          k3d version

      - name: Create UDS Cluster
        shell: bash
        run: |
          UDS_CONFIG=.github/config/uds-config.yaml make create-uds-gpu-cluster

      - name: Test UDS GPU Cluster
        run: |
          # Check NVIDIA plugin daemonset logs and container for issues
          uds zarf tools kubectl logs -n kube-system daemonset/nvidia-device-plugin-daemonset
          uds zarf tools kubectl exec -it daemonset/nvidia-device-plugin-daemonset -n kube-system -c nvidia-device-plugin-ctr -- nvidia-smi

          for node in $(uds zarf tools kubectl get nodes -o jsonpath='{.items[*].metadata.name}'); do
            echo "Checking GPU resources on node: $node"
            uds zarf tools kubectl describe node $node
            echo "Checking taints on node: $node"
            uds zarf tools kubectl describe node $node | grep Taints
          done

          uds zarf tools kubectl get runtimeclass
          uds zarf tools kubectl logs -n kube-system daemonset/nvidia-device-plugin-daemonset
          uds zarf tools journalctl -u kubelet

          # Use a CUDA test pod in-cluster to ensure GPUs can be scheduled
          uds zarf tools kubectl apply -f packages/k3d-gpu/test/cuda-vector-add.yaml

          while [[ $(uds zarf tools kubectl get pod -l app=gpu-pod --namespace=default -o jsonpath='{.items[*].status.phase}') != "Succeeded" ]]; do
              echo "Waiting for pod to complete..."
              sleep 10
          done

          uds zarf tools kubectl logs -l app=gpu-pod --namespace=default

      - name: Setup API and Supabase
        uses: ./.github/actions/lfai-core

      #######
      # vllm
      #######
      - name: Deploy vLLM
        run: |
          make build-vllm LOCAL_VERSION=e2e-test DOCKER_FLAGS="--build-arg MAX_CONTEXT_LENGTH=500"

          make local-registry
          make sdk-wheel LOCAL_VERSION=e2e-test
          docker build --build-arg MAX_CONTEXT_LENGTH=500 --build-arg LOCAL_VERSION=e2e-test -t ghcr.io/defenseunicorns/leapfrogai/vllm:e2e-test -f packages/vllm/Dockerfile .
          docker tag ghcr.io/defenseunicorns/leapfrogai/vllm:e2e-test localhost:5000/defenseunicorns/leapfrogai/vllm:e2e-test
          docker push localhost:5000/defenseunicorns/leapfrogai/vllm:e2e-test

          uds zarf package create packages/vllm --flavor upstream -o packages/vllm --registry-override=ghcr.io=localhost:5000 --insecure --set IMAGE_VERSION=e2e-test --confirm

          docker image prune -af

          uds zarf package deploy packages/vllm/zarf-package-vllm-amd64-e2e-test.tar.zst -l=trace --confirm
          rm packages/vllm/zarf-package-vllm-amd64-e2e-test.tar.zst

          # Check on pods being scheduled on GPUs
          uds zarf tools kubectl get pods \
            --all-namespaces \
            --output=yaml \
            | uds zarf tools yq eval -o=json '
              ["Pod", "Namespace", "Container", "GPU"] as $header |
              [$header] + [
                .items[] |
                .metadata as $metadata |
                .spec.containers[] |
                select(.resources.requests["nvidia.com/gpu"]) |
                [
                  $metadata.name,
                  $metadata.namespace,
                  .name,
                  .resources.requests["nvidia.com/gpu"]
                ]
              ]' - \
            | uds zarf tools yq -r '(.[0] | @tsv), (.[1:][] | @tsv)' \
            | column -t -s $'\t'

          # Check vLLM deployment logs for issues
          while [[ $(uds zarf tools kubectl get pod -l app=lfai-vllm --namespace=leapfrogai -o jsonpath='{.items[*].status.phase}') != "Running" ]]; do
              echo "Waiting for pod to be ready..."
              sleep 10
          done

          uds zarf tools kubectl logs -n leapfrogai deployment/vllm-model

      - name: Test vLLM
        env:
          MODEL_NAME: vllm
        run: |
          python -m pytest ./tests/e2e/test_llm_generation.py -vv
