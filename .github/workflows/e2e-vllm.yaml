# End-to-end testing that deploys and tests Supabase, API, UI, and VLLM

name: e2e-vllm
on:
  pull_request:
    types:
      - opened            # default trigger
      - reopened          # default trigger
      - synchronize       # default trigger
      - ready_for_review  # don't run on draft PRs
      - milestoned        # allows us to trigger on bot PRs
    paths:
      # Catch-all
      - "**"

      # Ignore updates to the .github directory, unless it's this current file
      - "!.github/**"
      - ".github/workflows/e2e-vllm.yaml"
      - ".github/actions/uds-cluster/action.yaml"

      # Ignore docs and website things
      - "!**.md"
      - "!docs/**"
      - "!adr/**"
      - "!website/**"
      - "!netlify.toml"

      # Ignore updates to generic github metadata files
      - "!CODEOWNERS"
      - "!.gitignore"
      - "!LICENSE"

      # Ignore local development files
      - "!.pre-commit-config.yaml"

      # Ignore non e2e tests changes
      - "!tests/pytest/**"

      # Ignore LFAI-UI source code changes
      - "!src/leapfrogai_ui/**"

      # Ignore changes to unrelated packages
      - "!packages/k3d-gpu/**"
      - "!packages/llama-cpp-python/**"
      - "!packages/repeater/**"
      - "!packages/text-embeddings/**"
      - "!packages/ui/**"
      - "!packages/whisper/**"

concurrency:
  group: e2e-vllm-${{ github.ref }}
  cancel-in-progress: true

jobs:
  e2e_vllm:
    runs-on: ai-linux-x64-gpu
    if: ${{ !github.event.pull_request.draft }}

    steps:
      - name: Checkout Repo
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

      - name: Setup and Check NVIDIA Dependencies
        run: |
          # Check the NVIDIA driver and container toolkit versions
          nvidia-smi
          nvidia-ctk --version

          # Configure Docker with the NVIDIA Container Toolkit
          sudo nvidia-ctk runtime configure --runtime=docker
          sudo systemctl restart docker

          # Check Docker GPU info and access
          docker info | grep nvidia
          docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi

      - name: Setup Python
        uses: ./.github/actions/python
        with:
          additionalOptionalDep: dev-vllm

      - name: Setup UDS Environment
        uses: defenseunicorns/uds-common/.github/actions/setup@822dac4452e6815aadcf09f487406ff258756a0c # v0.14.0
        with:
          registry1Username: ${{ secrets.IRON_BANK_ROBOT_USERNAME }}
          registry1Password: ${{ secrets.IRON_BANK_ROBOT_PASSWORD }}
          ghToken: ${{ secrets.GITHUB_TOKEN }}
          udsCliVersion: 0.14.0
        continue-on-error: true # see next step for permissions issues workarounds

      - name: Setup UDS Workaround
        env:
          UDS_VERSION: v0.14.0
        run: |
          # Use root permissions to allow installation of UDS CLI binary
          sudo wget -O uds https://github.com/defenseunicorns/uds-cli/releases/download/${{ env.UDS_VERSION }}/uds-cli_${{ env.UDS_VERSION }}_Linux_amd64 && \
          sudo chmod +x uds && \
          sudo chown $(whoami) uds

          # Change ownership of binaries to current runner user
          sudo mv uds /usr/local/bin/
          sudo chown $(whoami) /usr/local/bin/k3d

          # Check runner user access to the binaries
          uds version
          k3d version

      - name: Create UDS Cluster
        shell: bash
        run: |
          UDS_CONFIG=.github/config/uds-config.yaml make create-uds-gpu-cluster

      - name: Test UDS GPU Cluster
        run: |
          # Check NVIDIA plugin daemonset logs for issues
          uds zarf tools kubectl logs -n kube-system daemonset/nvidia-device-plugin-daemonset

          # Use CUDA tests in-cluster to ensure GPUs can be scheduled
          uds zarf tools kubectl apply -f packages/k3d-gpu/test/cuda-vector-add.yaml
          uds zarf tools wait-for -n default pod/gpu-pod && \
            uds zarf tools kubectl logs -n default pod/gpu-pod

      - name: Setup LFAI-API and Supabase
        uses: ./.github/actions/lfai-core

      #######
      # vllm
      #######
      - name: Deploy vLLM
        run: |
          make build-vllm LOCAL_VERSION=e2e-test
          docker image prune -af
          uds zarf package deploy packages/vllm/zarf-package-vllm-amd64-e2e-test.tar.zst -l=trace --confirm
          rm packages/vllm/zarf-package-vllm-amd64-e2e-test.tar.zst

      - name: Test vLLM
        env:
          MODEL_NAME: vllm
        run: |
          python -m pytest ./tests/e2e/test_llm_generation.py -vv
