variables:
  leapfrogai-ui:
    domain: https://ai.uds.dev
    model: llama-cpp-python
    concurrent_requests: "false"
    ai4ns_branding: "false"
    leapfrogai_rag_url: http://rag.leapfrogai.svc.cluster.local:8000
    max_tokens: 8192
  
  llama-cpp-python:
    requests_cpu: 0
    limits_cpu: 0
    requests_memory: 0
    limits_memory: 0
    gpu_enabled: "false"
    requests_gpu: 0
    limits_gpu: 0

  text-embeddings:
    requests_cpu: 0
    limits_cpu: 0
    requests_memory: 0
    limits_memory: 0
    gpu_enabled: "false"
    requests_gpu: 0
    limits_gpu: 0

  whisper:
    requests_cpu: 0
    limits_cpu: 0
    requests_memory: 0
    limits_memory: 0
    gpu_enabled: "false"
    requests_gpu: 0
    limits_gpu: 0

  # rag:
  #   model: llama-cpp-python
  #   ssl_verification: "false" # if certs exist in-cluster, make true
  #   response_mode: "raw" # default mode for query endpoint
  #   temperature: 0 # refine method temperature for vllm
  #   max_output: 2048
  #   context_window: 4096
  #   chunk_size: 512
  #   overlap_size: 64
  #   embedding_model_name: text-embeddings
  #   top_k: 20