variables:
  leapfrogai-ui:
    domain: https://ai.uds.dev
    model: llama-cpp-python
    concurrent_requests: "false"
    ai4ns_branding: "false"
    leapfrogai_rag_url: http://rag.leapfrogai.svc.cluster.local:8000
    max_tokens: 8192
  
  llama-cpp-python:
    requests_cpu: 0
    limits_cpu: 0
    requests_memory: 0
    limits_memory: 0
    requests_gpu: 0
    limits_gpu: 0

  text-embeddings:
    requests_cpu: 0
    limits_cpu: 0
    requests_memory: 0
    limits_memory: 0
    requests_gpu: 0
    limits_gpu: 0

  whisper:
    requests_cpu: 0
    limits_cpu: 0
    requests_memory: 0
    limits_memory: 0
    requests_gpu: 0
    limits_gpu: 0

  rag:
    model: llama-cpp-python
    ssl_verification: "false" # if certs exist in-cluster, make true
    response_mode: "raw" # default mode for query endpoint
    temperature: 0 # refine method temperature for vllm
    max_output: 2048
    context_window: 4096
    chunk_size: 512
    overlap_size: 64
    embedding_model_name: text-embeddings
    top_k: 20