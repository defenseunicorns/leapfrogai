apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "chart.fullname" . }}
  namespace: {{ .Release.Namespace | default "leapfrogai" }}
  labels:
    {{- include "chart.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  {{- with .Values.strategy }}
  strategy:
    {{- toYaml . | nindent 8 }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "chart.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        app: lfai-{{ .Values.nameOverride }}
        {{- include "chart.selectorLabels" . | nindent 8 }}
    spec:
      # handle the case where limit is a number or a string, else comparison fails on helm install
      {{- $gpuLimit := index .Values.resources.limits "nvidia.com/gpu" }}
      {{- if kindIs "string" $gpuLimit }}
        {{- $gpuLimit = $gpuLimit | float64 }}
      {{- end }}
      {{- if gt $gpuLimit 0.0 }}
      runtimeClassName: nvidia
      {{- else if .Values.gpu.runtimeClassName }}
      runtimeClassName: {{ .Values.gpu.runtimeClassName }}
      {{- end }}
      # It's necessary to include the ###ZARF_DATA_INJECTION_MARKER### somewhere in the podspec, otherwise data injections will not occur.
      initContainers:
        - name: data-loader
          image: cgr.dev/chainguard/bash:latest
          securityContext:
            {{- toYaml .Values.modelInjectionContainer.securityContext | nindent 12 }}
          # This command looks for the Zarf "data injection marker" which is a timestamped file that is injected after everything else and marks the injection as complete.
          command:
            [
              "sh",
              "-c",
              'while [ ! -f /data/.model/###ZARF_DATA_INJECTION_MARKER### ]; do echo "waiting for zarf data sync" && sleep 1; done; echo "we are done waiting!"',
            ]
          resources:
            {{- toYaml .Values.modelInjectionContainer.resources | nindent 12 }}
          volumeMounts:
            - name: leapfrogai-pv-storage
              mountPath: "/data"
      volumes:
        - name: leapfrogai-pv-storage
          persistentVolumeClaim:
            claimName: lfai-{{ .Values.nameOverride }}-pv-claim
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          {{- with .Values.env }}
          env:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          env:
            - name: LFAI_MODEL_PATH
              value: "/data/.model"
            - name: GPU_REQUEST
              value: "{{ (index .Values.resources.limits "nvidia.com/gpu") | default "0" }}"
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          volumeMounts:
            - name: leapfrogai-pv-storage
              mountPath: "/data"
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
